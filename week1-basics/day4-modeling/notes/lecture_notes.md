# lecture_notes

# Day 4 강의 노트

**강의**: [P] Part 2 - Chapter 1 (01-15 ~ 01-18)

**날짜**: 2026.02.12

---

## 01-15. Model Training (21분)

### 핵심 개념

### 모델 학습이란?

- 정의: 데이터의 형태에 맞게 모델은 선별하고, 구하고자 하는 답학습 하는경우도 있고 안하는 경우도 있지만
  최종적인 목적은 그 답을 추출하는 패턴을 학습 - 보완
          거의 맞아요! 더 정확하게:

          "데이터에서 입력(X)과 출력(y)의
          관계(패턴)를 찾아 가중치(W)를 업데이트하는 과정"

- 백엔드 비유: 클라이언트가 요청한 응답을 내놓는 과정
  - 보완
    수많은 요청 로그를 분석해서
    "이런 패턴의 요청은 이렇게 처리하면 된다"는
    규칙을 자동으로 만드는 과정

### Train / Validation / Test 분리

- Train: 학습용 데이터 (가장 많은 비율차지)
- Validation: 검증용 데이터 (없거나 보통 테스트랑 같은 비율차지)
- Test: 테스트(실질적으로 해당 모델이 잘 동작하는지 확인하는 데이터 )

```
전체 데이터
├── Train (80)%   → 모델 학습용
├── Validation (10)%  → 튜닝용
└── Test (10)%    → 최종 평가용
```

### Overfitting vs Underfitting

| 상태         | 의미          | 증상                            | 해결 방법           |
| ------------ | ------------- | ------------------------------- | ------------------- |
| Overfitting  | 과적합        | Train 성능 높음, Test 성능 낮음 | 데이터 수를 늘린다  |
| Underfitting | 과소적합      | Train 성능 낮음, Test 성능 높음 | 소수데이터를 늘린다 |
| 적절         | ??음 좋은상태 |                                 |                     |

**백엔드 비유**:

- Overfitting =
  - 보완
    특정 환경에서만 동작하는 코드
    예: 로컬에서만 테스트해서
    "내 컴퓨터에서는 되는데요?" 상황
    → 실제 서버(Test)에서는 에러!
- Underfitting =모르겟음
  - 보완
    너무 단순한 로직
    예: 모든 요청에 "200 OK"만 반환
    → 학습 자체를 안 한 것

### Epoch / Batch / Iteration

- Epoch: 학습횟수 ⇒ 전체 데이터를 몇 번 반복 학습?
- Batch: 한번의 데이터를 나눌때의 단위? ⇒ 한 번에 몇 개씩 학습?
- Iteration: 잘 모르겠음 ⇒ 한 Epoch에 몇 번 업데이트?
  공식:
  Iteration = 전체 데이터 수 / Batch Size

```
예시:
데이터 1000개, Batch Size 100, Epoch 10이면?
- 1 Epoch = 1000 / 100 = 10 Iteration
- 전체 학습 = 10 * 10 100Iteration
잘모르겠음,,
```

### 💡 강의에서 인상 깊었던 것

-
-

### ❓ 이해 안 된 부분

- Iteration에 대해서 잘몰랐지만 다시 한 번 공부
-

---

## 01-16. Model Evaluation (29분)

### 핵심 개념

### Confusion Matrix

```
                 예측: Positive  예측: Negative
실제: Positive       TP              FN
실제: Negative       FP              TN
```

- **TP** (True Positive):
- **FP** (False Positive):
- **FN** (False Negative):
- **TN** (True Negative):

**타이타닉 예시**:

- TP: 생존 예측 → 실제 생존
- FP: 생존 예측 → 실제 생존 아님
- FN: 생존 아니라고 예측→ 실제 생존
- TN: 생존 아니라고 예측 → 실제 생존아님

### 평가 지표

| 지표      | 공식              | 의미                           | 언제 중요?                          |
| --------- | ----------------- | ------------------------------ | ----------------------------------- |
| Accuracy  | (TP+TN) / 전체    | 전체 갯수중에 맞은갯수         | 클래스 균형잡힌 단순 분류           |
| Precision | TP / (TP+FP)      | 내가 맞앗다고 한것중 맞은 갯수 | FP 비용이 클 때 (스팸 분류)         |
| Recall    | TP / (TP+FN)      | 전체 정답중 맞춘 갯수          | FN 비용이 클 때 (암 진단, 사기탐지) |
| F1-Score  | 2 * (P*R) / (P+R) | precision , recall조화롭게     | 둘 다 중요할 때 (불균형 데이터)     |

F1-Score 언제?:
→ Precision이랑 Recall 둘 다 포기 못할 때!
→ 불균형 데이터에서 종합 점수로 쓰기 좋음
→ Day 3 Class Imbalance + 평가지표 연결!

### Accuracy의 함정

- 문제 상황: 정답 갯수가 적을때 다 맞다고 하면 점수는 좋아져버림
- 예시: 100개중 10개가 타겟인데 그냥 다 맞다고 하면 90%가 나와버리는 문제
- 해결: 해당 문제는 Accuracy로 하지 않고 recall로 한다??
- 보완
  맞아요! 더 정확하게:
  상황에 따라:
  - 사기 탐지 → Recall (놓치면 안 됨)
  - 스팸 분류 → Precision (정상 메일 차단 방지)
  - 둘 다 중요 → F1-Score
  "Accuracy 버리고 F1-Score 쓴다"
  → 불균형 데이터의 기본 공식!

**Day 3 연결**:

```
Class Imbalance (95:5) 에서
Accuracy = 95% 인데 왜 나쁜 모델인가?

→ 100개중 5개가 타겟인데 그냥 다 맞다고 하면  95%가 나와버리는 문제
```

### Precision vs Recall 트레이드오프

```
Precision 중요한 경우:
→ FP가 비용이 클 때
→ 예시: 스팸메일 분류

Recall 중요한 경우:
→ FN이 비용이 클 때
→ 예시: 암환자 검색
```

**금융 이상 탐지 연결** (프로젝트 3):

```
사기 탐지에서는 Precision vs Recall
어느 게 더 중요한가?
→Recall
이유: 하나라도 놓치면 뼈아픔 차라리 아니더라고 한번 더 확인하는게 맞다고봄
```

### Cross Validation

- 개념: 여러번 검증 하는거
- 왜 필요한가: 현재 학습이 운으로 한게 아니라 조금씩 데이터를 변경하면서 검증
- K-Fold: 데이터를 k갯수로 나누는거??
  - 보완
    K-Fold 더 정확하게:
    데이터를 K개로 나눠서
    K번 돌아가며 검증셋을 바꿔가며 학습
    예시 (K=5):
    1회: [검증][학습][학습][학습][학습]
    2회: [학습][검증][학습][학습][학습]
    3회: [학습][학습][검증][학습][학습]
    ...
    → 5번의 결과 평균 = 최종 성능
    → "운 좋게 좋은 데이터만 골랐다"는 문제 해결!
    백엔드 비유:
    여러 환경(개발/스테이징/운영)에서
    각각 테스트해서 평균 성능 확인하는 것

### 💡 강의에서 인상 깊었던 것

- 무조건 재현성이 높아야하고 운으로 하는게 아닌 재검증이 중요하구나
-

### ❓ 이해 안 된 부분

- K번 돌아가며 이부분을 생각못했는데 보완에서 확인
-

---

## 01-17. Model HyperParameter Tuning (12분)

### 핵심 개념

### Parameter vs HyperParameter

| 구분           | 정의       | 예시           | 누가 결정? |
| -------------- | ---------- | -------------- | ---------- |
| Parameter      | 잘모르겠음 | 가중치(Weight) | 잘모르겠음 |
| HyperParameter | 잘모르겠음 | 잘모르겠음     | 잘모르겠음 |

**백엔드 비유**:

- Parameter =잘모르겠음
- HyperParameter =잘모르겠음

### 보완

핵심 차이:

Parameter:
→ 모델이 학습하면서 스스로 찾는 값
→ 예: 가중치(Weight), 편향(Bias)
→ 누가 결정? 모델이 자동으로!

HyperParameter:
→ 사람이 학습 전에 미리 설정하는 값
→ 예: n_estimators=100, max_depth=5
→ 누가 결정? 개발자가!

백엔드 비유:
Parameter = DB가 자동으로 최적화하는 인덱스
HyperParameter = 개발자가 설정하는 커넥션 풀 크기, 타임아웃

→ 커넥션 풀을 10으로 할지 100으로 할지
= n_estimators를 10으로 할지 100으로 할지
완전히 같은 개념!

### Tuning 방법

| 방법          | 설명           | 장점                                                         | 단점                          |
| ------------- | -------------- | ------------------------------------------------------------ | ----------------------------- |
| Grid Search   | 모든 조합 시도 | 하나도 놓치지않고 모든 조합을 하여 최적의 파라미터 조합 색출 | 시간과 돈을 그만큼 사용해야함 |
| Random Search | 랜덤 조합 시도 | 잘모르겠음                                                   | 중요한 조합을 놓칠수도잇음    |
| Bayesian      | 이전 결과 반영 | 잘모르겠음                                                   | 잘모르겠음                    |

**1️⃣ Grid Search**

| 항목 | 설명                                  |
| ---- | ------------------------------------- |
| 방식 | 모든 조합을 전부 탐색                 |
| 장점 | 최적 조합을 절대 놓치지 않음          |
| 단점 | 조합 수가 폭발적으로 증가 (지수 증가) |

**2️⃣ Random Search**

| 항목 | 설명                                   |
| ---- | -------------------------------------- |
| 방식 | 랜덤으로 조합 선택                     |
| 장점 | 적은 연산으로 좋은 조합 찾을 확률 높음 |
| 단점 | 최적 조합을 100% 보장하지는 않음       |

**3️⃣ Bayesian Optimization**

| 항목 | 설명                                          |
| ---- | --------------------------------------------- |
| 방식 | 이전 실험 결과를 기반으로 다음 탐색 지점 결정 |
| 장점 | 훨씬 적은 실험으로 최적점 근접                |
| 단점 | 구현 복잡, 계산 비용 있음                     |

Random Search 장점:
→ Grid Search 대비 시간 대폭 절약
→ 의외로 좋은 조합을 빨리 찾음
→ 실무에서 Grid보다 자주 씀!

Bayesian 장점:
→ 이전 결과를 보고 다음 탐색 방향 결정
→ "여기는 별로였으니 저쪽 탐색"
→ 가장 효율적이지만 구현 복잡

실무 선택:
빠른 실험 → Random Search
최적화 중요 → Bayesian

### RandomForest HyperParameter 예시

```python
RandomForestClassifier(
    n_estimators=__,   # 나무 개수
    max_depth=__,      # 나무 최대 깊이
    min_samples_split=__,  # 분기 최소 샘플 수
    random_state=42
)
```

- n_estimators 크면: 분기가 너무 많아지는데 그에 효과는 잘모르겟음 그냥 속도가 느려지려나
  - 보완
    - 성능 ↑ (보통)
    - 계산량 ↑
    - 과적합 ↓ (오히려 줄어듦)

- max_depth 크면: 사소한거까지 학습을 해서 과적합 문제가 생길듯
  - 보완
    - 모델 복잡 ↑
    - 과적합 위험 ↑

### 💡 강의에서 인상 깊었던 것

- 단순히 모델 성능이 좋아지는것만 생각하면 안되는듯
-

### ❓ 이해 안 된 부분

-
-

---

## 01-18. Model Selection (9분)

### 핵심 개념

### 모델 선택 기준

```
단순히 성능만 보면 안 된다!

고려할 것:
1. 성능 (Accuracy, F1-Score)
2. 데이터의 타입?? 회귀로 가야하냐 분류로 가야하냐
3.
4.
5.
```

### 모델별 특징

| 모델                | 장점                                                | 단점              | 적합한 경우 |
| ------------------- | --------------------------------------------------- | ----------------- | ----------- |
| Logistic Regression | 빠름, 해석 쉬움                                     | 비선형 어려움     |             |
| Decision Tree       | Random Forest의 열악판 이라고는 알고잇는데 자세히는 | 과적합 쉬움       |             |
| Random Forest       | 어떤건지는 알겟는데 뭐가 좋고 나쁜지 모르겠음       | 느림, 해석 어려움 |             |

### 보완

Logistic Regression:

- 장점: 빠름, 해석 가능, 가볍다
- 단점: 비선형 패턴 못 잡음
- 적합: 선형 관계, 빠른 베이스라인
- ⚠️ 회귀 문제 아님! 분류 모델이에요
  (이름이 Regression이라 헷갈리지만!)

Decision Tree:

- 장점: 해석 쉬움, 시각화 가능
- 단점: 과적합 매우 쉬움
- 적합: 규칙 기반 설명 필요할 때

Random Forest:

- 장점: 과적합 방지, 성능 좋음
- 단점: 느림, 해석 어려움
- 원리: Decision Tree 여러 개 투표
  "민주주의 모델" ← 다수결!

### 실무에서 모델 선택하는 방법

1. 성능 (F1-Score, Accuracy)
2. 속도 (추론 시간 - 서빙 관점!)
3. 해석 가능성 (금융/의료는 필수)
4. 데이터 크기 (작으면 단순 모델)
5. 유지보수 (팀이 이해할 수 있나?)

**백엔드 비유**:

```
모델 선택 = 라이브러리/프레임워크 선택

Express vs Nest.js 고를 때처럼
성능만이 아니라 유지보수, 팀 역량도 고려
```

### 💡 강의에서 인상 깊었던 것

-
-

### ❓ 이해 안 된 부분

-
-

---

## 📊 오늘 강의 전체 연결고리

```
Day 3에서 배운 것          Day 4에서 배운 것
─────────────────────────────────────────
Class Imbalance      →     F1-Score 중요성
Stratified Sampling  →     train_test_split stratify
데이터 전처리         →     모델 입력 준비
                           ↓
                     Day 5에서 할 것
                     Docker로 모델 패키징
                     Week 2에서 할 것
                     FastAPI로 API 서빙
```

---

## 🔍 스스로 찾아보기 (강의 후 추가)

### [VS] joblib vs pickle

- 발견한 차이:
- ML에서 joblib 쓰는 이유:

### [WHAT IF] random_state 없애면?

- 실험 결과:
- 결론:

### [WHY] F1-Score vs Accuracy

- Accuracy 함정 예시:
- 실무 선택 기준:

---

## 💬 오늘 강의 한 줄 요약

> (강의 다 듣고 작성)

---

## ✏️ 핵심 질문 5개 (강의 끝나고 작성)

> 답변 작성 후 Claude한테 보내서 피드백 받기!

### Q1. Overfitting을 백엔드 개발 경험으로 비유해서 설명하면?

**A**: Train에서만 좋고 막상 실제로는 원하는 성적을 못냄

---

### Q2. Accuracy가 90%인데 왜 나쁜 모델일 수 있나? 구체적인 예시로 설명하면?

**A**: 데이터가 불균형일때 그냥 다 맞다고 해버리면 소수클래스는 0%를 감지

---

### Q3. Precision과 Recall 중 금융 사기 탐지에서 더 중요한 건 무엇이고 이유는?

**A**: Recall 놓치면 뼈아픔, 차라리 한 번 더 확인 FN(사기인데 정상) 비용 > FP(정상인데 사기) 비용

---

### Q4. HyperParameter와 Parameter의 차이를 신입 개발자에게 설명한다면?

**A**: Parameter는 모델이 알아서 정함(가중치, 편향) , HyperParameter 개발자가 직접 설정 각 모델에 대응되는

---

### Q5. 모델 선택 시 성능(F1-Score) 외에 실무에서 고려해야 할 것들은?

**A**: 속도, 해석이 되냐 , 데이터크기 , 유지보수 가능성

---

**Status**: 수강 중 🔄

**Next**: 실습 → `train_model.py` 작성
